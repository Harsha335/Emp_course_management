{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPLOYEE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file (assumed to be already extracted)\n",
    "employee_data = pd.read_csv('./staging/Employee.csv')   # change thiss---------------------\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_employee_data = employee_data[['emp_id', 'designation']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_employee_data = cleaned_employee_data.drop_duplicates(subset='emp_id')\n",
    "\n",
    "# Step 4: One-Hot Encoding for the designation column\n",
    "cleaned_employee_data = pd.get_dummies(cleaned_employee_data, columns=['designation'], prefix='designation', drop_first=True)\n",
    "\n",
    "# Step 5: Convert boolean columns to integers (0, 1)\n",
    "bool_columns = cleaned_employee_data.columns[1:]  # Assuming the first column is emp_id\n",
    "cleaned_employee_data[bool_columns] = cleaned_employee_data[bool_columns].astype(int)\n",
    "\n",
    "# Step 6: Provide information about the cleaned table\n",
    "print(cleaned_employee_data.info())\n",
    "print(cleaned_employee_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_employee_data.to_csv('./prep/cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COURSE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file\n",
    "courses_data = pd.read_csv('./staging/Course.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_courses_data = courses_data[['course_id', 'duration', 'difficulty_level']]\n",
    "\n",
    "# Step 3: Perform ordinal encoding for difficulty_level\n",
    "difficulty_order = ['BASIC', 'BEGINNER', 'INTERMEDIATE', 'EXPERT']  # Define the order of difficulty levels\n",
    "cleaned_courses_data['difficulty_level'] = pd.Categorical(cleaned_courses_data['difficulty_level'], \n",
    "                                                            categories=difficulty_order, \n",
    "                                                            ordered=True)\n",
    "cleaned_courses_data['difficulty_level'] = cleaned_courses_data['difficulty_level'].cat.codes  # Convert to codes\n",
    "\n",
    "# Step 4: Convert duration to weeks\n",
    "def duration_to_weeks(duration):\n",
    "    if 'months' in duration:\n",
    "        return int(duration.split()[0]) * 4  # Assuming 1 month = 4 weeks\n",
    "    elif 'years' in duration:\n",
    "        return int(duration.split()[0]) * 52  # Assuming 1 year = 52 weeks\n",
    "    elif 'weeks' in duration:\n",
    "        return int(duration.split()[0])\n",
    "    else:\n",
    "        return 1  # Handle any unexpected format\n",
    "\n",
    "cleaned_courses_data['duration_in_weeks'] = cleaned_courses_data['duration'].apply(duration_to_weeks)\n",
    "\n",
    "# Step 1: Calculate the mean and standard deviation\n",
    "mean_duration = cleaned_courses_data['duration_in_weeks'].mean()\n",
    "std_duration = cleaned_courses_data['duration_in_weeks'].std()\n",
    "\n",
    "# Step 2: Apply Z-score normalization\n",
    "cleaned_courses_data['standardized_duration'] = (cleaned_courses_data['duration_in_weeks'] - mean_duration) / std_duration\n",
    "\n",
    "\n",
    "# Step 5: Clean the DataFrame by dropping the original duration column\n",
    "cleaned_courses_data = cleaned_courses_data.drop(columns=['duration'])\n",
    "\n",
    "# Step 6: Provide information about the cleaned table\n",
    "print(cleaned_courses_data.info())\n",
    "print(cleaned_courses_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_courses_data.to_csv('./prep/cleaned_courses_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     16\u001b[0m emp_courses \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     17\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memp_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39magg({\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Step 4: Encoding categorical variables\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m learning_path_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_path_name\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Create DataFrame for encoded features\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = pd.read_csv('../DataEngineering/reporting/merged.csv')\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "data.dropna(subset=['emp_id', 'course_id', 'learning_path_name'], inplace=True)\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "# Aggregate courses completed per employee\n",
    "emp_courses = (\n",
    "    data.groupby('emp_id')\n",
    "    .agg({\n",
    "        'course_id': 'nunique',  # Number of unique courses completed\n",
    "        'completion_rate': 'mean',  # Average completion rate\n",
    "        'test_score_normalized': 'mean',  # Average test score\n",
    "        'time_spent_in_sec': 'sum',  # Total time spent\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Encoding categorical variables\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "learning_path_encoded = encoder.fit_transform(data[['learning_path_name']])\n",
    "\n",
    "# Create DataFrame for encoded features\n",
    "learning_path_df = pd.DataFrame(learning_path_encoded, columns=encoder.get_feature_names_out())\n",
    "emp_courses = pd.concat([emp_courses, learning_path_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = emp_courses.drop(['emp_id'], axis=1)\n",
    "y = data.groupby('emp_id')['learning_path_name'].first()  # Assuming we take the first for simplicity\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Implementation\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can tune n_neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get predictions for all employees\n",
    "emp_courses['predicted_learning_path'] = knn.predict(X_scaled)\n",
    "\n",
    "# Final Output\n",
    "best_learning_paths = emp_courses[['emp_id', 'predicted_learning_path']].drop_duplicates()\n",
    "best_learning_paths.to_csv('./best_learning_paths.csv', index=False)\n",
    "\n",
    "print(best_learning_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query user ID: JMD001 (Name: Harsha)\n",
      "\n",
      "Nearest Employees for User ID JMD001 (Name: Harsha):\n",
      "\n",
      "Self Match (Distance = 0):\n",
      "User ID JMD196 (Name: Juana Upton), with distance of 0.3378\n",
      "User ID JMD199 (Name: Colin Friesen PhD), with distance of 0.3956\n",
      "User ID JMD130 (Name: Jim Crist Sr.), with distance of 0.6333\n",
      "User ID JMD122 (Name: Mrs. Sharon Jones), with distance of 0.6362\n",
      "User ID JMD197 (Name: Dr. Duane Rutherford), with distance of 0.6378\n",
      "\n",
      "Recommended Courses for User ID JMD001 (Name: Harsha) (not previously taken):\n",
      "\n",
      "IT trainer Fundamentals\n",
      "Clinical research associate Fundamentals\n",
      "Pathologist Fundamentals\n",
      "Commissioning editor Fundamentals\n",
      "Designer, ceramics/pottery Fundamentals\n",
      "Chief Financial Officer Fundamentals\n",
      "Industrial/product designer Fundamentals\n",
      "Senior tax professional/tax inspector Fundamentals\n",
      "Aeronautical engineer Fundamentals\n",
      "Engineer, automotive Fundamentals\n",
      "Waste management officer Fundamentals\n",
      "\n",
      "RMSE: 0.1895\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "df = pd.read_csv('../DataEngineering/reporting/merged.csv')\n",
    " \n",
    " \n",
    "# %%\n",
    "df.columns\n",
    " \n",
    "# %% [markdown]\n",
    "# FEATURE ENGINEERING AND NORMALIZING\n",
    " \n",
    " \n",
    "# %%\n",
    "df['raw_score'] = df['completion_rate'] * df['test_score_normalized']\n",
    " \n",
    "# Step 2: Normalize the score\n",
    "max_score = df['raw_score'].max()\n",
    "min_score = df['raw_score'].min()\n",
    " \n",
    "# Normalization formula: (score - min) / (max - min) * new_max\n",
    "df['normalized_score'] = (df['raw_score'] - min_score) / (max_score - min_score)\n",
    "df.head()\n",
    " \n",
    "# %%\n",
    "required_columns= [\n",
    "    'emp_id','emp_name','course_id','course_name','normalized_score'\n",
    "]\n",
    " \n",
    "df_final=df[required_columns]\n",
    "df_final.head()\n",
    " \n",
    "len(df_final)\n",
    " \n",
    "# %%\n",
    "df_final.drop_duplicates\n",
    "len(df_final)\n",
    " \n",
    "# %%\n",
    "df_final_agg = df_final.groupby(['emp_id', 'course_name'], as_index=False)['normalized_score'].mean()\n",
    " \n",
    "# Create a pivot table: emp_id as rows, course_name as columns, and normalized_score as values\n",
    "df_pivot = df_final_agg.pivot(index='emp_id', columns='course_name', values='normalized_score').fillna(0)\n",
    " \n",
    "# Convert the pivot table to a sparse matrix\n",
    "df_matrix = csr_matrix(df_pivot.values)\n",
    " \n",
    "# Fit the NearestNeighbors model\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model_knn.fit(df_matrix)\n",
    " \n",
    "# Randomly choose an employee for recommendations\n",
    "query_index = np.random.choice(df_pivot.shape[0])\n",
    "query_emp_id = df_pivot.index[query_index]\n",
    " \n",
    "# Get the name of the querying user\n",
    "query_user_name = df_final[df_final['emp_id'] == query_emp_id]['emp_name'].values[0]\n",
    "print(f\"Query user ID: {query_emp_id} (Name: {query_user_name})\")\n",
    " \n",
    "# Get recommendations\n",
    "distances, indices = model_knn.kneighbors(df_pivot.iloc[query_index, :].values.reshape(1, -1), n_neighbors=6)\n",
    " \n",
    "# Display nearest employees\n",
    "print(f'\\nNearest Employees for User ID {query_emp_id} (Name: {query_user_name}):\\n')\n",
    "recommended_ids = []\n",
    "for i in range(len(distances.flatten())):\n",
    "    if i == 0:\n",
    "        print('Self Match (Distance = 0):')\n",
    "    else:\n",
    "        recommended_id = df_pivot.index[indices.flatten()[i]]\n",
    "        recommended_ids.append(recommended_id)\n",
    "        user_name = df_final[df_final['emp_id'] == recommended_id]['emp_name'].values[0]  # Get the name\n",
    "        print(f'User ID {recommended_id} (Name: {user_name}), with distance of {distances.flatten()[i]:.4f}')\n",
    " \n",
    "# Gather courses from nearest employees\n",
    "all_courses = set()\n",
    "user_courses = set(df_final[df_final['emp_id'] == query_emp_id]['course_name'])\n",
    " \n",
    "for emp_id in recommended_ids:\n",
    "    courses_taken = df_final[df_final['emp_id'] == emp_id]['course_name'].unique()\n",
    "    all_courses.update(courses_taken)\n",
    " \n",
    "# Determine courses to recommend\n",
    "unique_courses = all_courses.difference(user_courses)\n",
    " \n",
    "if unique_courses:\n",
    "    print(f'\\nRecommended Courses for User ID {query_emp_id} (Name: {query_user_name}) (not previously taken):\\n')\n",
    "    for course in unique_courses:\n",
    "        print(course)\n",
    "else:\n",
    "    # If no unique courses, suggest any course from the nearest employees\n",
    "    print(f'\\nAll courses have been taken by User ID {query_emp_id} (Name: {query_user_name}). Suggesting courses from nearest employees:\\n')\n",
    "    suggested_courses = list(all_courses)\n",
    "    for course in suggested_courses:\n",
    "        print(course)\n",
    " \n",
    "# Optionally calculate RMSE (if needed)\n",
    "def calculate_rmse(recommended_ids, actual_scores):\n",
    "    relevant_scores = df_final[df_final['emp_id'].isin(recommended_ids)]\n",
    " \n",
    "    if relevant_scores.empty:\n",
    "        return float('nan')  # Return NaN if no relevant scores are found\n",
    " \n",
    "    y_true = relevant_scores['normalized_score']\n",
    "    y_pred = relevant_scores['normalized_score'].mean()  # Using the mean as a simple prediction\n",
    " \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, [y_pred] * len(y_true)))\n",
    "    return rmse\n",
    " \n",
    "# Calculate RMSE (if desired)\n",
    "rmse_value = calculate_rmse(recommended_ids, df_final)\n",
    "print(f'\\nRMSE: {rmse_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emp_id        learning_path_name  interest_score\n",
      "0    JMD001                  Frontend        0.670000\n",
      "2    JMD002                Full Stack        0.765714\n",
      "3    JMD003                Full Stack        0.278997\n",
      "11   JMD100      Software Engineering        0.435167\n",
      "17   JMD101  Internet of Things (IoT)        0.376000\n",
      "..      ...                       ...             ...\n",
      "597  JMD195              Data Science        0.603000\n",
      "600  JMD196       Agile Methodologies        0.705000\n",
      "613  JMD197     Software Architecture        0.531667\n",
      "617  JMD198        Project Management        0.830000\n",
      "622  JMD199                  Big Data        0.611000\n",
      "\n",
      "[103 rows x 3 columns]\n",
      "Accuracy: 0.013071895424836602\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      " AI and Machine Learning       0.04      0.11      0.06         9\n",
      "     Agile Methodologies       0.00      0.00      0.00        11\n",
      "                Big Data       0.00      0.00      0.00         4\n",
      "              Blockchain       0.00      0.00      0.00        13\n",
      "         Cloud Computing       0.00      0.00      0.00        12\n",
      "           Cybersecurity       0.00      0.00      0.00         9\n",
      "            Data Science       0.00      0.00      0.00         9\n",
      "                  DevOps       0.00      0.00      0.00         9\n",
      "              Full Stack       0.00      0.00      0.00         1\n",
      "Internet of Things (IoT)       0.25      0.07      0.11        15\n",
      "      Mobile Development       0.00      0.00      0.00        12\n",
      "      Project Management       0.00      0.00      0.00        12\n",
      "       Quality Assurance       0.00      0.00      0.00         9\n",
      "   Software Architecture       0.00      0.00      0.00        13\n",
      "    Software Engineering       0.00      0.00      0.00         8\n",
      "         Web Development       0.00      0.00      0.00         7\n",
      "\n",
      "                accuracy                           0.01       153\n",
      "               macro avg       0.02      0.01      0.01       153\n",
      "            weighted avg       0.03      0.01      0.01       153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../DataEngineering/reporting/merged.csv')\n",
    "# Select relevant columns\n",
    "features = ['completion_rate', 'test_score_normalized', 'success_rate']\n",
    "target = 'learning_path_name'\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df.dropna(subset=[target], inplace=True)\n",
    "# Group by emp_id and learning path, calculating mean scores\n",
    "learning_path_interest = df.groupby(['emp_id', 'learning_path_name'])[features].mean().reset_index()\n",
    "# Example: Calculating a simple interest score\n",
    "learning_path_interest['interest_score'] = (\n",
    "    learning_path_interest['completion_rate'] * 0.5 +\n",
    "    learning_path_interest['test_score_normalized'] * 0.3 +\n",
    "    learning_path_interest['success_rate'] * 0.2\n",
    ")\n",
    "# Get the learning path with the highest interest score for each employee\n",
    "best_learning_paths = learning_path_interest.loc[\n",
    "    learning_path_interest.groupby('emp_id')['interest_score'].idxmax()\n",
    "]\n",
    "print(best_learning_paths[['emp_id', 'learning_path_name', 'interest_score']])\n",
    "best_learning_paths.to_csv('best_learning_paths.csv', index=False)\n",
    "# One-hot encode the learning_path_name\n",
    "# df_encoded = pd.get_dummies(df, columns=['learning_path_name'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[features]  # Features\n",
    "y = df['learning_path_name']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
