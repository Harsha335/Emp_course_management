{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction and Loading Steps\n",
    "\n",
    "1. **Data Extraction**:\n",
    "   - **Connect to the Database**: Use appropriate libraries (e.g., `psycopg2` for PostgreSQL) to establish a connection.\n",
    "   - **Retrieve Data**: Write SQL queries to extract the necessary tables or data subsets.\n",
    "   - **Export Data**: Optionally, save the extracted data into CSV files for further processing.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - **Load Data into DataFrames**: Use libraries like `pandas` to load the extracted CSV files or data directly from the database into DataFrames for manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas  psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: _prisma_migrations\n",
      "Table _prisma_migrations exported to staging\\_prisma_migrations.csv\n",
      "Exporting table: Employee\n",
      "Table Employee exported to staging\\Employee.csv\n",
      "Exporting table: Course\n",
      "Table Course exported to staging\\Course.csv\n",
      "Exporting table: CourseEnrollment\n",
      "Table CourseEnrollment exported to staging\\CourseEnrollment.csv\n",
      "Exporting table: User\n",
      "Table User exported to staging\\User.csv\n",
      "Exporting table: QuestionBank\n",
      "Table QuestionBank exported to staging\\QuestionBank.csv\n",
      "Exporting table: Questions\n",
      "Table Questions exported to staging\\Questions.csv\n",
      "Exporting table: CourseEngageLogs\n",
      "Table CourseEngageLogs exported to staging\\CourseEngageLogs.csv\n",
      "Exporting table: Notifications\n",
      "Table Notifications exported to staging\\Notifications.csv\n",
      "Exporting table: LearningPathMap\n",
      "Table LearningPathMap exported to staging\\LearningPathMap.csv\n",
      "Exporting table: LearningPath\n",
      "Table LearningPath exported to staging\\LearningPath.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_38068\\4164182136.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_38068\\4164182136.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Define your database connection parameters\n",
    "db_params = {\n",
    "    'database': 'Emp_course_management',\n",
    "    'user': 'postgres',\n",
    "    'password': '965335',\n",
    "    'host': 'localhost',  # or your database host\n",
    "    'port': '5432'  # Default PostgreSQL port\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Fetch all table names from the public schema\n",
    "cur.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema='public';\n",
    "\"\"\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "# Define staging directory\n",
    "staging_dir = 'staging'\n",
    "os.makedirs(staging_dir, exist_ok=True)  # Create staging directory if it doesn't exist\n",
    "\n",
    "# Loop through each table and export to CSV\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Exporting table: {table_name}\")\n",
    "    \n",
    "    # Read table into a DataFrame\n",
    "    df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
    "    \n",
    "    # Define the path for the CSV file\n",
    "    csv_file_path = os.path.join(staging_dir, f\"{table_name}.csv\")\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Table {table_name} exported to {csv_file_path}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps\n",
    "\n",
    "1. **Remove Duplicates**: \n",
    "   - Identify and remove duplicate records to ensure data integrity.\n",
    "\n",
    "2. **Handle Missing Values**:\n",
    "   - Decide on a strategy for missing data (e.g., imputation, removal, or using a placeholder).\n",
    "   - Implement the strategy based on your analysis needs.\n",
    "\n",
    "3. **Data Type Conversion**:\n",
    "   - Ensure all columns have the correct data types (e.g., integers, floats, dates).\n",
    "   - Convert categorical variables to a suitable format (e.g., using one-hot encoding).\n",
    "\n",
    "4. **Standardization and Normalization**:(data science)\n",
    "   - Standardize numerical columns to a common scale, if necessary.\n",
    "   - Normalize data for specific algorithms that require it.\n",
    "\n",
    "### Feature Engineering and Data Preparation Steps\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Create new features that may be beneficial for prediction (e.g., extracting year from a date, combining features).\n",
    "   - Encode categorical variables using techniques like label encoding or one-hot encoding.(data science)\n",
    "\n",
    "2. **Aggregation and Grouping**:\n",
    "   - Aggregate data to a desired level (e.g., total sales per month).\n",
    "   - Group data based on relevant categories to simplify analysis.\n",
    "\n",
    "3. **Outlier Detection and Treatment**:\n",
    "   - Identify and handle outliers based on domain knowledge or statistical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   emp_id       103 non-null    string\n",
      " 1   email        103 non-null    string\n",
      " 2   emp_name     103 non-null    string\n",
      " 3   designation  103 non-null    string\n",
      "dtypes: string(4)\n",
      "memory usage: 3.3 KB\n",
      "None\n",
      "   emp_id                 email        emp_name                    designation\n",
      "0  JMD001  harsha@jmangroup.com          Harsha              SOFTWARE_ENGINEER\n",
      "1  JMD002  pardhu@jmangroup.com          pardhu           SR_SOFTWARE_ENGINEER\n",
      "2  JMD003   akhil@jmangroup.com           Akhil               SOLUTION_ENABLER\n",
      "3  JMD100  JMD100@jmangroup.com    Guy Reichert  TECHNOLOGY_SOLUTION_ARCHITECT\n",
      "4  JMD101  JMD101@jmangroup.com  Gerald Nicolas   PRINCIPAL_SOLUTION_ARCHITECT\n"
     ]
    }
   ],
   "source": [
    "# EMPLOYEE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file (assumed to be already extracted)\n",
    "employee_data = pd.read_csv('./staging/Employee.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_employee_data = employee_data[['emp_id', 'email', 'emp_name', 'designation']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_employee_data = cleaned_employee_data.drop_duplicates(subset='emp_id')\n",
    "\n",
    "# step 4: Change datatype\n",
    "cleaned_employee_data['emp_id'] = cleaned_employee_data['emp_id'].astype('string')\n",
    "cleaned_employee_data['email'] = cleaned_employee_data['email'].astype('string')\n",
    "cleaned_employee_data['emp_name'] = cleaned_employee_data['emp_name'].astype('string')\n",
    "cleaned_employee_data['designation'] = cleaned_employee_data['designation'].astype('string')\n",
    "\n",
    "# Step 5: Provide information about the cleaned table\n",
    "print(cleaned_employee_data.info())\n",
    "print(cleaned_employee_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_employee_data.to_csv('./prep/cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   course_id                 103 non-null    int64 \n",
      " 1   course_name               103 non-null    object\n",
      " 2   course_description        103 non-null    object\n",
      " 3   course_difficulty_level   103 non-null    object\n",
      " 4   course_duration_in_weeks  103 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None\n",
      "   course_id                            course_name  \\\n",
      "0        100      Commissioning editor Fundamentals   \n",
      "1        101              Neurosurgeon Fundamentals   \n",
      "2        102      Merchandiser, retail Fundamentals   \n",
      "3        103  Arts development officer Fundamentals   \n",
      "4        104    Embryologist, clinical Fundamentals   \n",
      "\n",
      "                                  course_description course_difficulty_level  \\\n",
      "0  Loss give employee ball. Eye level popular app...            INTERMEDIATE   \n",
      "1  She change picture. Produce owner voice if. Di...                  EXPERT   \n",
      "2  Change finish production realize president. Op...                BEGINNER   \n",
      "3  Shoulder pressure subject notice determine sis...            INTERMEDIATE   \n",
      "4     Interview huge agree. They wish analysis city.                BEGINNER   \n",
      "\n",
      "   course_duration_in_weeks  \n",
      "0                        20  \n",
      "1                         4  \n",
      "2                         8  \n",
      "3                        12  \n",
      "4                        12  \n"
     ]
    }
   ],
   "source": [
    "# COURSE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file\n",
    "courses_data = pd.read_csv('./staging/Course.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_courses_data = courses_data[['course_id', 'course_name', 'description', 'duration', 'difficulty_level']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_courses_data = cleaned_courses_data.drop_duplicates(subset='course_id')\n",
    "\n",
    "# Step 3: Convert duration to weeks\n",
    "def duration_to_weeks(duration):\n",
    "    if 'months' in duration:\n",
    "        return int(duration.split()[0]) * 4  # Assuming 1 month = 4 weeks\n",
    "    elif 'years' in duration:\n",
    "        return int(duration.split()[0]) * 52  # Assuming 1 year = 52 weeks\n",
    "    elif 'weeks' in duration:\n",
    "        return int(duration.split()[0])\n",
    "    else:\n",
    "        return 1  # Handle any unexpected format\n",
    "\n",
    "cleaned_courses_data['duration_in_weeks'] = cleaned_courses_data['duration'].apply(duration_to_weeks)\n",
    "\n",
    "# Step 5: Clean the DataFrame by dropping the original duration column\n",
    "cleaned_courses_data = cleaned_courses_data.drop(columns=['duration'])\n",
    "\n",
    "# step 6: changing column name\n",
    "cleaned_courses_data.rename(columns={'description': 'course_description', 'difficulty_level' : 'course_difficulty_level', 'duration_in_weeks' : 'course_duration_in_weeks'}, inplace=True)\n",
    "\n",
    "# Step 7: Provide information about the cleaned table\n",
    "print(cleaned_courses_data.info())\n",
    "print(cleaned_courses_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_courses_data.to_csv('./prep/cleaned_courses_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   learning_path_id           19 non-null     int64 \n",
      " 1   learning_path_description  19 non-null     object\n",
      " 2   learning_path_name         19 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 588.0+ bytes\n",
      "None\n",
      "   learning_path_id                          learning_path_description  \\\n",
      "0                 1  A machine learning (ML) learning path is a str...   \n",
      "1                 2  An Artificial Intelligence (AI) learning path ...   \n",
      "2                 3  The Full Stack Learning Path equips learners w...   \n",
      "3                 4  The Frontend Learning Path focuses on the desi...   \n",
      "4               100    Master the fundamentals of software development   \n",
      "\n",
      "        learning_path_name  \n",
      "0         Machine Learning  \n",
      "1  Artificial Intelligence  \n",
      "2               Full Stack  \n",
      "3                 Frontend  \n",
      "4     Software Engineering  \n"
     ]
    }
   ],
   "source": [
    "# LEARNING_PATH - TABLE\n",
    "learning_path_data = pd.read_csv('./staging/LearningPath.csv')\n",
    "\n",
    "cleaned_learningPath = learning_path_data[['learning_path_id', 'description', 'path_name']]\n",
    "\n",
    "cleaned_learningPath = cleaned_learningPath.drop_duplicates(subset='learning_path_id')\n",
    "\n",
    "cleaned_learningPath.rename(columns={'path_name' : 'learning_path_name', 'description' : 'learning_path_description'}, inplace=True)\n",
    "\n",
    "print(cleaned_learningPath.info())\n",
    "print(cleaned_learningPath.head())\n",
    "\n",
    "cleaned_learningPath.to_csv('./prep/cleaned_learning_paths.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262 entries, 0 to 261\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   course_id         262 non-null    int64\n",
      " 1   learning_path_id  262 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.2 KB\n",
      "None\n",
      "   course_id  learning_path_id\n",
      "0         12                 1\n",
      "1         12                 2\n",
      "2         10                 3\n",
      "3         10                 4\n",
      "4          9                 3\n"
     ]
    }
   ],
   "source": [
    "# LearningPathMap - TABLE\n",
    "\n",
    "learning_path_map_data = pd.read_csv('./staging/LearningPathMap.csv')\n",
    "\n",
    "cleaned_learningPathMap = learning_path_map_data[['course_id', 'learning_path_id']]\n",
    "\n",
    "cleaned_learningPathMap = cleaned_learningPathMap.drop_duplicates(subset=['course_id', 'learning_path_id'], keep='first')\n",
    "\n",
    "print(cleaned_learningPathMap.info())\n",
    "print(cleaned_learningPathMap.head())\n",
    "\n",
    "cleaned_learningPathMap.to_csv('./prep/cleaned_learning_paths_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 306 entries, 0 to 305\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   enroll_id               306 non-null    int64  \n",
      " 1   emp_id                  306 non-null    object \n",
      " 2   course_id               306 non-null    int64  \n",
      " 3   current_page            306 non-null    int64  \n",
      " 4   total_pages             306 non-null    int64  \n",
      " 5   test_score              305 non-null    float64\n",
      " 6   course_certificate_url  226 non-null    object \n",
      " 7   createdAt               306 non-null    object \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 19.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 306 entries, 0 to 305\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   enroll_id                     306 non-null    int64  \n",
      " 1   emp_id                        306 non-null    object \n",
      " 2   course_id                     306 non-null    int64  \n",
      " 3   createdAt                     306 non-null    object \n",
      " 4   course_certificate_generated  306 non-null    bool   \n",
      " 5   completion_rate               306 non-null    float64\n",
      " 6   test_score_normalized         306 non-null    float64\n",
      "dtypes: bool(1), float64(2), int64(2), object(2)\n",
      "memory usage: 14.8+ KB\n",
      "None\n",
      "   enroll_id  emp_id  course_id                createdAt  \\\n",
      "0          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "1          2  JMD001          9  2024-08-08 10:07:51.855   \n",
      "2          4  JMD001         10  2024-08-08 10:07:51.855   \n",
      "3          5  JMD003          9  2024-10-08 09:55:54.078   \n",
      "4        100  JMD100        187  2023-11-23 13:25:43.324   \n",
      "\n",
      "   course_certificate_generated  completion_rate  test_score_normalized  \n",
      "0                          True         1.000000                   0.60  \n",
      "1                         False         0.250784                   0.10  \n",
      "2                          True         0.800000                   0.70  \n",
      "3                         False         0.557994                   0.00  \n",
      "4                          True         0.970000                   0.09  \n"
     ]
    }
   ],
   "source": [
    "Course_Enrollment_data = pd.read_csv('./staging/CourseEnrollment.csv')\n",
    "\n",
    "cleaned_course_enrollment_data = Course_Enrollment_data[['enroll_id', 'emp_id', 'course_id', 'current_page', 'total_pages', 'test_score', 'course_certificate_url', 'createdAt']]\n",
    "\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop_duplicates(subset=['enroll_id', 'course_id'], keep='first')\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "# Replace missing values without using inplace\n",
    "cleaned_course_enrollment_data['current_page'] = cleaned_course_enrollment_data['current_page'].fillna(0)\n",
    "cleaned_course_enrollment_data['total_pages'] = cleaned_course_enrollment_data['total_pages'].fillna(100)\n",
    "cleaned_course_enrollment_data['test_score'] = cleaned_course_enrollment_data['test_score'].fillna(0)\n",
    "# Create a new boolean column 'course_certificate_generated'\n",
    "cleaned_course_enrollment_data['course_certificate_generated'] = cleaned_course_enrollment_data['course_certificate_url'].apply(lambda x: True if isinstance(x, str) and x.strip() else False)\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop(columns=['course_certificate_url'])\n",
    "\n",
    "# Normalize current_page based on total_pages\n",
    "cleaned_course_enrollment_data['completion_rate'] = cleaned_course_enrollment_data['current_page'] / cleaned_course_enrollment_data['total_pages']\n",
    "# Normalize test_score (assuming the max score is 100)\n",
    "cleaned_course_enrollment_data['test_score_normalized'] = cleaned_course_enrollment_data['test_score'] / 100\n",
    "\n",
    "cleaned_course_enrollment_data.drop(columns=['current_page', 'total_pages', 'test_score'],axis=1, inplace=True)\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "print(cleaned_course_enrollment_data.head())\n",
    "\n",
    "cleaned_course_enrollment_data.to_csv('./prep/cleaned_courseEnrollment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   enroll_id          767 non-null    int64 \n",
      " 1   start_time         767 non-null    object\n",
      " 2   time_spent_in_sec  767 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.1+ KB\n",
      "None\n",
      "   enroll_id               start_time  time_spent_in_sec\n",
      "0          2  2024-10-07 04:33:08.446                  8\n",
      "1          4  2024-10-07 04:33:22.656                  4\n",
      "2          3  2024-10-07 05:08:09.332                  4\n",
      "3          5  2024-10-08 09:14:53.354                 10\n",
      "4          3  2023-10-23 21:03:47.279               4191\n"
     ]
    }
   ],
   "source": [
    "# CourseEngageLogs - TABLE  \n",
    "\n",
    "CourseEngageLogs = pd.read_csv('./staging/CourseEngageLogs.csv')\n",
    "cleaned_course_engageLogs_data = CourseEngageLogs[['enroll_id', 'start_time', 'time_spent_in_sec']]\n",
    "\n",
    "cleaned_course_engageLogs_data = cleaned_course_engageLogs_data.drop_duplicates(subset=['enroll_id', 'start_time'], keep='first')\n",
    "\n",
    "print(cleaned_course_engageLogs_data.info())\n",
    "print(cleaned_course_engageLogs_data.head())\n",
    "\n",
    "cleaned_course_engageLogs_data.to_csv('./prep/cleaned_courseEngageLogs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1165 entries, 0 to 1164\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   notification_id  1165 non-null   int64 \n",
      " 1   enroll_id        1165 non-null   int64 \n",
      " 2   status           1164 non-null   object\n",
      " 3   user_viewed      1165 non-null   bool  \n",
      " 4   created_date     1165 non-null   object\n",
      "dtypes: bool(1), int64(2), object(2)\n",
      "memory usage: 37.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1165 entries, 0 to 1164\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   notification_id     1165 non-null   int64 \n",
      " 1   enroll_id           1165 non-null   int64 \n",
      " 2   certificate_status  1165 non-null   bool  \n",
      " 3   user_viewed         1165 non-null   bool  \n",
      " 4   created_date        1165 non-null   object\n",
      "dtypes: bool(2), int64(2), object(1)\n",
      "memory usage: 29.7+ KB\n",
      "None\n",
      "   notification_id  enroll_id  certificate_status  user_viewed  \\\n",
      "0             1183          3               False        False   \n",
      "1             1184          3               False        False   \n",
      "2             1185          3                True        False   \n",
      "3             1186          2               False        False   \n",
      "4             1187          2               False        False   \n",
      "\n",
      "              created_date  \n",
      "0  2024-03-11 05:36:43.918  \n",
      "1  2023-11-20 22:46:04.493  \n",
      "2  2024-10-08 19:42:04.746  \n",
      "3  2024-03-18 18:47:11.576  \n",
      "4  2023-11-04 19:12:26.495  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_38068\\3034790909.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "#  Notifications - TABLE\n",
    "\n",
    "notifications_data = pd.read_csv('./staging/Notifications.csv')\n",
    "\n",
    "cleaned_notifications_data = notifications_data[['notification_id', 'enroll_id', 'status', 'user_viewed', 'created_date']]\n",
    "\n",
    "print(cleaned_notifications_data.info())    # status contains null - admin to taken a desition (make it into false)\n",
    "\n",
    "cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n",
    "cleaned_notifications_data.rename(columns={'status': 'certificate_status'}, inplace=True)\n",
    "\n",
    "print(cleaned_notifications_data.info())    \n",
    "print(cleaned_notifications_data.head())\n",
    "\n",
    "cleaned_notifications_data.to_csv('./prep/cleaned_Notifications.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration & Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration and Storage Steps\n",
    "\n",
    "1. **Join Tables**:\n",
    "   - Merge or join different tables to create a unified dataset that includes all necessary features for analysis.\n",
    "   - Ensure that the join keys are appropriate and that the merging process retains the relevant data.\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Create Final Tables:\n",
    "     - Organize the cleaned and transformed data into final tables that are structured for analysis and modeling.\n",
    "     - Save these final tables as CSV files or store them in a database for easy access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   enroll_id  emp_id  course_id                createdAt  \\\n",
      "0          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "1          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "2          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "3          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "4          3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "\n",
      "   course_certificate_generated  completion_rate  test_score_normalized  \\\n",
      "0                          True              1.0                    0.6   \n",
      "1                          True              1.0                    0.6   \n",
      "2                          True              1.0                    0.6   \n",
      "3                          True              1.0                    0.6   \n",
      "4                          True              1.0                    0.6   \n",
      "\n",
      "                  email emp_name           designation  ...  \\\n",
      "0  pardhu@jmangroup.com   pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "1  pardhu@jmangroup.com   pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "2  pardhu@jmangroup.com   pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "3  pardhu@jmangroup.com   pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "4  pardhu@jmangroup.com   pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "\n",
      "  course_duration_in_weeks learning_path_id  \\\n",
      "0                        6                3   \n",
      "1                        6                3   \n",
      "2                        6                3   \n",
      "3                        6                3   \n",
      "4                        6                3   \n",
      "\n",
      "                           learning_path_description  learning_path_name  \\\n",
      "0  The Full Stack Learning Path equips learners w...          Full Stack   \n",
      "1  The Full Stack Learning Path equips learners w...          Full Stack   \n",
      "2  The Full Stack Learning Path equips learners w...          Full Stack   \n",
      "3  The Full Stack Learning Path equips learners w...          Full Stack   \n",
      "4  The Full Stack Learning Path equips learners w...          Full Stack   \n",
      "\n",
      "                start_time time_spent_in_sec notification_id  \\\n",
      "0  2024-10-07 05:08:09.332                 4            1183   \n",
      "1  2024-10-07 05:08:09.332                 4            1184   \n",
      "2  2024-10-07 05:08:09.332                 4            1185   \n",
      "3  2024-10-07 05:08:09.332                 4               4   \n",
      "4  2024-10-07 05:08:09.332                 4              10   \n",
      "\n",
      "  certificate_status  user_viewed             created_date  \n",
      "0              False        False  2024-03-11 05:36:43.918  \n",
      "1              False        False  2023-11-20 22:46:04.493  \n",
      "2               True        False  2024-10-08 19:42:04.746  \n",
      "3               True         True  2024-10-07 05:08:31.476  \n",
      "4              False        False  2024-05-03 20:29:23.545  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "cleaned_employee_data = pd.read_csv('prep/cleaned_employee_data.csv')\n",
    "cleaned_course_enrollment_data = pd.read_csv('prep/cleaned_courseEnrollment.csv')\n",
    "cleaned_courses_data = pd.read_csv('prep/cleaned_courses_data.csv')\n",
    "cleaned_learning_paths_map = pd.read_csv('prep/cleaned_learning_paths_map.csv')\n",
    "cleaned_learning_paths_data = pd.read_csv('prep/cleaned_learning_paths.csv')\n",
    "cleaned_course_engage_logs = pd.read_csv('prep/cleaned_courseEngageLogs.csv')\n",
    "cleaned_notifications = pd.read_csv('prep/cleaned_Notifications.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  total_attempts  accepted_attempts  success_rate\n",
      "0            2              12                  0      0.000000\n",
      "1            3               7                  3      0.428571\n",
      "2            4              10                  3      0.300000\n",
      "3            5               4                  0      0.000000\n",
      "4          100               3                  1      0.333333\n",
      "..         ...             ...                ...           ...\n",
      "301        397               5                  1      0.200000\n",
      "302        398               5                  1      0.200000\n",
      "303        399               2                  1      0.500000\n",
      "304        400               3                  1      0.333333\n",
      "305        401               4                  1      0.250000\n",
      "\n",
      "[306 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "success_rate_df = cleaned_notifications.groupby('enroll_id').agg(\n",
    "    total_attempts=('certificate_status', 'size'),  # Total attempts\n",
    "    accepted_attempts=('certificate_status', lambda x: x.sum()),  # Count of accepted attempts\n",
    ").reset_index()\n",
    "\n",
    "# Calculate success rate\n",
    "success_rate_df['success_rate'] = success_rate_df['accepted_attempts'] / success_rate_df['total_attempts']\n",
    "\n",
    "# Display the results\n",
    "print(success_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  time_spent_in_sec\n",
      "0            2              18397\n",
      "1            3              11087\n",
      "2            4               7997\n",
      "3            5              10178\n",
      "4          100               4674\n",
      "..         ...                ...\n",
      "301        397               5316\n",
      "302        398               8157\n",
      "303        399               4585\n",
      "304        400               8217\n",
      "305        401               3228\n",
      "\n",
      "[306 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by enroll_id and calculate total time spent\n",
    "total_time_spent_df = cleaned_course_engage_logs.groupby('enroll_id')['time_spent_in_sec'].sum().reset_index()\n",
    "\n",
    "# Display the results\n",
    "print(total_time_spent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  emp_id  course_id                createdAt  \\\n",
      "0            3  JMD002          9  2024-10-08 09:55:54.078   \n",
      "1            2  JMD001          9  2024-08-08 10:07:51.855   \n",
      "2            4  JMD001         10  2024-08-08 10:07:51.855   \n",
      "3            4  JMD001         10  2024-08-08 10:07:51.855   \n",
      "4            5  JMD003          9  2024-10-08 09:55:54.078   \n",
      "..         ...     ...        ...                      ...   \n",
      "760        400  JMD199        131  2023-10-12 06:37:04.322   \n",
      "761        401  JMD199        144  2024-07-15 23:21:37.903   \n",
      "762        401  JMD199        144  2024-07-15 23:21:37.903   \n",
      "763        401  JMD199        144  2024-07-15 23:21:37.903   \n",
      "764        401  JMD199        144  2024-07-15 23:21:37.903   \n",
      "\n",
      "     course_certificate_generated  completion_rate  test_score_normalized  \\\n",
      "0                            True         1.000000                   0.60   \n",
      "1                           False         0.250784                   0.10   \n",
      "2                            True         0.800000                   0.70   \n",
      "3                            True         0.800000                   0.70   \n",
      "4                           False         0.557994                   0.00   \n",
      "..                            ...              ...                    ...   \n",
      "760                          True         0.100000                   0.71   \n",
      "761                          True         0.780000                   0.57   \n",
      "762                          True         0.780000                   0.57   \n",
      "763                          True         0.780000                   0.57   \n",
      "764                          True         0.780000                   0.57   \n",
      "\n",
      "                    email           emp_name           designation  ...  \\\n",
      "0    pardhu@jmangroup.com             pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "1    harsha@jmangroup.com             Harsha     SOFTWARE_ENGINEER  ...   \n",
      "2    harsha@jmangroup.com             Harsha     SOFTWARE_ENGINEER  ...   \n",
      "3    harsha@jmangroup.com             Harsha     SOFTWARE_ENGINEER  ...   \n",
      "4     akhil@jmangroup.com              Akhil      SOLUTION_ENABLER  ...   \n",
      "..                    ...                ...                   ...  ...   \n",
      "760  JMD199@jmangroup.com  Colin Friesen PhD  SR_SOFTWARE_ENGINEER  ...   \n",
      "761  JMD199@jmangroup.com  Colin Friesen PhD  SR_SOFTWARE_ENGINEER  ...   \n",
      "762  JMD199@jmangroup.com  Colin Friesen PhD  SR_SOFTWARE_ENGINEER  ...   \n",
      "763  JMD199@jmangroup.com  Colin Friesen PhD  SR_SOFTWARE_ENGINEER  ...   \n",
      "764  JMD199@jmangroup.com  Colin Friesen PhD  SR_SOFTWARE_ENGINEER  ...   \n",
      "\n",
      "                                    course_description  \\\n",
      "0    A MERN stack course covers the integration of ...   \n",
      "1    A MERN stack course covers the integration of ...   \n",
      "2    A React.js course typically covers the fundame...   \n",
      "3    A React.js course typically covers the fundame...   \n",
      "4    A MERN stack course covers the integration of ...   \n",
      "..                                                 ...   \n",
      "760                        There may new edge mission.   \n",
      "761  Right doctor although. Decade million standard...   \n",
      "762  Right doctor although. Decade million standard...   \n",
      "763  Right doctor although. Decade million standard...   \n",
      "764  Right doctor although. Decade million standard...   \n",
      "\n",
      "    course_difficulty_level course_duration_in_weeks  learning_path_id  \\\n",
      "0              INTERMEDIATE                        6                 3   \n",
      "1              INTERMEDIATE                        6                 3   \n",
      "2                  BEGINNER                        2                 3   \n",
      "3                  BEGINNER                        2                 4   \n",
      "4              INTERMEDIATE                        6                 3   \n",
      "..                      ...                      ...               ...   \n",
      "760                  EXPERT                       24               114   \n",
      "761                BEGINNER                       16               113   \n",
      "762                BEGINNER                       16               102   \n",
      "763                BEGINNER                       16               109   \n",
      "764                BEGINNER                       16               114   \n",
      "\n",
      "                             learning_path_description  \\\n",
      "0    The Full Stack Learning Path equips learners w...   \n",
      "1    The Full Stack Learning Path equips learners w...   \n",
      "2    The Full Stack Learning Path equips learners w...   \n",
      "3    The Frontend Learning Path focuses on the desi...   \n",
      "4    The Full Stack Learning Path equips learners w...   \n",
      "..                                                 ...   \n",
      "760  Design scalable and maintainable software systems   \n",
      "761                  Handle and analyze large datasets   \n",
      "762     Analyze data and build machine learning models   \n",
      "763     Connect and control devices with IoT solutions   \n",
      "764  Design scalable and maintainable software systems   \n",
      "\n",
      "           learning_path_name time_spent_in_sec  total_attempts  \\\n",
      "0                  Full Stack             11087               7   \n",
      "1                  Full Stack             18397              12   \n",
      "2                  Full Stack              7997              10   \n",
      "3                    Frontend              7997              10   \n",
      "4                  Full Stack             10178               4   \n",
      "..                        ...               ...             ...   \n",
      "760     Software Architecture              8217               3   \n",
      "761                  Big Data              3228               4   \n",
      "762              Data Science              3228               4   \n",
      "763  Internet of Things (IoT)              3228               4   \n",
      "764     Software Architecture              3228               4   \n",
      "\n",
      "     accepted_attempts  success_rate  \n",
      "0                    3      0.428571  \n",
      "1                    0      0.000000  \n",
      "2                    3      0.300000  \n",
      "3                    3      0.300000  \n",
      "4                    0      0.000000  \n",
      "..                 ...           ...  \n",
      "760                  1      0.333333  \n",
      "761                  1      0.250000  \n",
      "762                  1      0.250000  \n",
      "763                  1      0.250000  \n",
      "764                  1      0.250000  \n",
      "\n",
      "[765 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the tables\n",
    "merged_data = (\n",
    "    cleaned_course_enrollment_data\n",
    "    .merge(cleaned_employee_data, on='emp_id', how='left')  # Join Employee Details with Course Enrollment\n",
    "    .merge(cleaned_courses_data, on='course_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(cleaned_learning_paths_map, on='course_id', how='left')  # Join Course Enrollment with Learning Path Mapping\n",
    "    .merge(cleaned_learning_paths_data, on='learning_path_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(total_time_spent_df, on='enroll_id', how='left')  # Join CourseEngageLogs\n",
    "    .merge(success_rate_df, on='enroll_id', how='left')  # Join Notifications\n",
    ")\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001            380704                 0.549271            0.426087   \n",
      "1  JMD002             77609                 1.000000            0.600000   \n",
      "2  JMD003             40712                 0.557994            0.000000   \n",
      "3  JMD100            185271                 0.314167            0.324167   \n",
      "4  JMD101            281290                 0.451064            0.326809   \n",
      "\n",
      "   certificates_generated  total_courses_completed  \n",
      "0                       0                      184  \n",
      "1                       0                       35  \n",
      "2                       0                       20  \n",
      "3                       0                       72  \n",
      "4                       0                      141  \n"
     ]
    }
   ],
   "source": [
    "# Group by emp_id to aggregate the features\n",
    "employee_performance = merged_data.groupby('emp_id').agg({\n",
    "    'time_spent_in_sec': 'sum',  # Total Time Spent\n",
    "    'completion_rate': 'mean',  # Average Course Completion\n",
    "    'test_score_normalized': 'mean',  # Average Test Score\n",
    "    'certificate_status': lambda x: (x == 'generated').sum(),  # Count of Generated Certificates\n",
    "    'course_difficulty_level': 'count'  # Count of Total Courses Completed\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "employee_performance.rename(columns={\n",
    "    'time_spent_in_sec': 'total_time_spent',\n",
    "    'completion_rate': 'average_completion_rate',\n",
    "    'test_score_normalized': 'average_test_score',\n",
    "    'certificate_status': 'certificates_generated',\n",
    "    'course_difficulty_level': 'total_courses_completed'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the employee performance data\n",
    "print(employee_performance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001            380704                 0.549271            0.426087   \n",
      "1  JMD002             77609                 1.000000            0.600000   \n",
      "2  JMD003             40712                 0.557994            0.000000   \n",
      "3  JMD100            185271                 0.314167            0.324167   \n",
      "4  JMD101            281290                 0.451064            0.326809   \n",
      "\n",
      "   certificates_generated  total_courses_completed  exam_success_rate  \n",
      "0                       0                      184                0.0  \n",
      "1                       0                       35                0.0  \n",
      "2                       0                       20                0.0  \n",
      "3                       0                       72                0.0  \n",
      "4                       0                      141                0.0  \n"
     ]
    }
   ],
   "source": [
    "# Calculate total attempts and success rate for each employee\n",
    "attempts = cleaned_notifications.groupby('enroll_id').agg({\n",
    "    'certificate_status': lambda x: (x == 'generated').sum()  # Count of accepted attempts\n",
    "}).reset_index()\n",
    "\n",
    "# Merge attempts with the employee performance data\n",
    "attempts = attempts.merge(cleaned_course_enrollment_data, on='enroll_id', how='left')\n",
    "attempts_grouped = attempts.groupby('emp_id').agg({\n",
    "    'certificate_status': 'sum',  # Total Successful Attempts\n",
    "    'enroll_id': 'count'  # Total Attempts\n",
    "}).rename(columns={\n",
    "    'certificate_status': 'successful_attempts',\n",
    "    'enroll_id': 'total_attempts'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate Exam Success Rate\n",
    "attempts_grouped['exam_success_rate'] = attempts_grouped['successful_attempts'] / attempts_grouped['total_attempts']\n",
    "\n",
    "# Merge the success rate with employee performance data\n",
    "employee_performance = employee_performance.merge(attempts_grouped[['emp_id', 'exam_success_rate']], on='emp_id', how='left')\n",
    "\n",
    "# Display the employee performance data with exam success rate\n",
    "print(employee_performance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001            380704                 0.549271            0.426087   \n",
      "1  JMD002             77609                 1.000000            0.600000   \n",
      "2  JMD003             40712                 0.557994            0.000000   \n",
      "3  JMD100            185271                 0.314167            0.324167   \n",
      "4  JMD101            281290                 0.451064            0.326809   \n",
      "\n",
      "   certificates_generated  total_courses_completed  exam_success_rate  \\\n",
      "0                       0                      184                0.0   \n",
      "1                       0                       35                0.0   \n",
      "2                       0                       20                0.0   \n",
      "3                       0                       72                0.0   \n",
      "4                       0                      141                0.0   \n",
      "\n",
      "   completed_courses_BEGINNER  completed_courses_EXPERT  \\\n",
      "0                         100                         0   \n",
      "1                           0                         0   \n",
      "2                           0                         0   \n",
      "3                           0                        36   \n",
      "4                          24                        45   \n",
      "\n",
      "   completed_courses_INTERMEDIATE  \n",
      "0                              84  \n",
      "1                              35  \n",
      "2                              20  \n",
      "3                              36  \n",
      "4                              72  \n"
     ]
    }
   ],
   "source": [
    "# Create a mapping of difficulty levels to counts\n",
    "difficulty_distribution = merged_data.groupby(['emp_id', 'course_difficulty_level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "difficulty_distribution.columns = [f'completed_courses_{level}' for level in difficulty_distribution.columns]\n",
    "\n",
    "# Merge difficulty distribution with employee performance data\n",
    "employee_performance = employee_performance.merge(difficulty_distribution, on='emp_id', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for completed courses\n",
    "employee_performance.fillna(0, inplace=True)\n",
    "\n",
    "# Display the final employee performance data\n",
    "print(employee_performance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\\n       ...\\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\\n      dtype='float64', length=103)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m learning_path_performance\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get the learning path with the highest combined score for each employee\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m best_learning_paths \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_path_performance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlearning_path_performance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memp_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midxmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m learning_path_performance\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Merge with learning path details to get descriptions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\\n       ...\\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\\n      dtype='float64', length=103)] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# Calculate the learning path predictions\n",
    "learning_path_performance = (\n",
    "    merged_data.groupby(['emp_id', 'learning_path_id'])\n",
    "    .agg({\n",
    "        'completion_rate': 'mean',  # Average completion percentage\n",
    "        'certificate_status': lambda x: (x == 'generated').sum()  # Total Certificates Generated\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the ratio or combined score\n",
    "learning_path_performance['combined_score'] = (\n",
    "    (learning_path_performance['completion_rate'] * 0.5) + \n",
    "    (learning_path_performance['certificate_status'] / learning_path_performance['certificate_status'].max() * 0.5)\n",
    ")\n",
    "\n",
    "# Get the learning path with the highest combined score for each employee\n",
    "best_learning_paths = learning_path_performance.loc[learning_path_performance.groupby('emp_id')['combined_score'].idxmax()]\n",
    "\n",
    "# Merge with learning path details to get descriptions\n",
    "best_learning_paths = best_learning_paths.merge(cleaned_learning_paths_data[['learning_path_id', 'learning_path_name']], on='learning_path_id', how='left')\n",
    "\n",
    "# Display the final recommendations\n",
    "print(best_learning_paths[['emp_id', 'learning_path_name', 'combined_score']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
