{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction and Loading Steps\n",
    "\n",
    "1. **Data Extraction**:\n",
    "   - **Connect to the Database**: Use appropriate libraries (e.g., `psycopg2` for PostgreSQL) to establish a connection.\n",
    "   - **Retrieve Data**: Write SQL queries to extract the necessary tables or data subsets.\n",
    "   - **Export Data**: Optionally, save the extracted data into CSV files for further processing.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - **Load Data into DataFrames**: Use libraries like `pandas` to load the extracted CSV files or data directly from the database into DataFrames for manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\assad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas  psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: LearningPathMap\n",
      "Table LearningPathMap exported to staging\\LearningPathMap.csv\n",
      "Exporting table: LearningPath\n",
      "Table LearningPath exported to staging\\LearningPath.csv\n",
      "Exporting table: CourseEnrollment\n",
      "Table CourseEnrollment exported to staging\\CourseEnrollment.csv\n",
      "Exporting table: User\n",
      "Table User exported to staging\\User.csv\n",
      "Exporting table: Questions\n",
      "Table Questions exported to staging\\Questions.csv\n",
      "Exporting table: QuestionBank\n",
      "Table QuestionBank exported to staging\\QuestionBank.csv\n",
      "Exporting table: CourseEngageLogs\n",
      "Table CourseEngageLogs exported to staging\\CourseEngageLogs.csv\n",
      "Exporting table: _prisma_migrations\n",
      "Table _prisma_migrations exported to staging\\_prisma_migrations.csv\n",
      "Exporting table: Employee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\assad\\AppData\\Local\\Temp\\ipykernel_28648\\2315815447.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
      "C:\\Users\\assad\\AppData\\Local\\Temp\\ipykernel_28648\\2315815447.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Employee exported to staging\\Employee.csv\n",
      "Exporting table: Notifications\n",
      "Table Notifications exported to staging\\Notifications.csv\n",
      "Exporting table: Course\n",
      "Table Course exported to staging\\Course.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Define your database connection parameters\n",
    "db_params = {\n",
    "    'database': 'Emp_Course_Management_System', #'Emp_course_management',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres', #'965335',\n",
    "    'host': 'localhost',  # or your database host\n",
    "    'port': '5432'  # Default PostgreSQL port\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Fetch all table names from the public schema\n",
    "cur.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema='public';\n",
    "\"\"\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "# Define staging directory\n",
    "staging_dir = 'staging'\n",
    "os.makedirs(staging_dir, exist_ok=True)  # Create staging directory if it doesn't exist\n",
    "\n",
    "# Loop through each table and export to CSV\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Exporting table: {table_name}\")\n",
    "    \n",
    "    # Read table into a DataFrame\n",
    "    df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
    "    \n",
    "    # Define the path for the CSV file\n",
    "    csv_file_path = os.path.join(staging_dir, f\"{table_name}.csv\")\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Table {table_name} exported to {csv_file_path}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps\n",
    "\n",
    "1. **Remove Duplicates**: \n",
    "   - Identify and remove duplicate records to ensure data integrity.\n",
    "\n",
    "2. **Handle Missing Values**:\n",
    "   - Decide on a strategy for missing data (e.g., imputation, removal, or using a placeholder).\n",
    "   - Implement the strategy based on your analysis needs.\n",
    "\n",
    "3. **Data Type Conversion**:\n",
    "   - Ensure all columns have the correct data types (e.g., integers, floats, dates).\n",
    "   - Convert categorical variables to a suitable format (e.g., using one-hot encoding).\n",
    "\n",
    "4. **Standardization and Normalization**:(data science)\n",
    "   - Standardize numerical columns to a common scale, if necessary.\n",
    "   - Normalize data for specific algorithms that require it.\n",
    "\n",
    "### Feature Engineering and Data Preparation Steps\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Create new features that may be beneficial for prediction (e.g., extracting year from a date, combining features).\n",
    "   - Encode categorical variables using techniques like label encoding or one-hot encoding.(data science)\n",
    "\n",
    "2. **Aggregation and Grouping**:\n",
    "   - Aggregate data to a desired level (e.g., total sales per month).\n",
    "   - Group data based on relevant categories to simplify analysis.\n",
    "\n",
    "3. **Outlier Detection and Treatment**:\n",
    "   - Identify and handle outliers based on domain knowledge or statistical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   emp_id       103 non-null    string\n",
      " 1   email        103 non-null    string\n",
      " 2   emp_name     103 non-null    string\n",
      " 3   designation  103 non-null    string\n",
      "dtypes: string(4)\n",
      "memory usage: 3.3 KB\n",
      "None\n",
      "   emp_id                 email       emp_name                    designation\n",
      "0  JMD001  harsha@jmangroup.com         Harsha              SOFTWARE_ENGINEER\n",
      "1  JMD002  pardhu@jmangroup.com         pardhu           SR_SOFTWARE_ENGINEER\n",
      "2  JMD003   akhil@jmangroup.com          Akhil               SOLUTION_ENABLER\n",
      "3  JMD100  JMD100@jmangroup.com   Mattie Koepp  TECHNOLOGY_SOLUTION_ARCHITECT\n",
      "4  JMD101  JMD101@jmangroup.com  Terry Fritsch   PRINCIPAL_SOLUTION_ARCHITECT\n"
     ]
    }
   ],
   "source": [
    "# EMPLOYEE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file (assumed to be already extracted)\n",
    "employee_data = pd.read_csv('./staging/Employee.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_employee_data = employee_data[['emp_id', 'email', 'emp_name', 'designation']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_employee_data = cleaned_employee_data.drop_duplicates(subset='emp_id')\n",
    "\n",
    "# step 4: Change datatype\n",
    "cleaned_employee_data['emp_id'] = cleaned_employee_data['emp_id'].astype('string')\n",
    "cleaned_employee_data['email'] = cleaned_employee_data['email'].astype('string')\n",
    "cleaned_employee_data['emp_name'] = cleaned_employee_data['emp_name'].astype('string')\n",
    "cleaned_employee_data['designation'] = cleaned_employee_data['designation'].astype('string')\n",
    "\n",
    "# Step 5: Provide information about the cleaned table\n",
    "print(cleaned_employee_data.info())\n",
    "print(cleaned_employee_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_employee_data.to_csv('./prep/cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   course_id                 103 non-null    int64 \n",
      " 1   course_name               103 non-null    object\n",
      " 2   course_description        103 non-null    object\n",
      " 3   course_difficulty_level   103 non-null    object\n",
      " 4   course_duration_in_weeks  103 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None\n",
      "   course_id                        course_name  \\\n",
      "0        100  Commissioning editor Fundamentals   \n",
      "1        101          Neurosurgeon Fundamentals   \n",
      "2          9                         MERN stack   \n",
      "3         10                            Reactjs   \n",
      "4         12                   Machine Learning   \n",
      "\n",
      "                                  course_description course_difficulty_level  \\\n",
      "0  Loss give employee ball. Eye level popular app...            INTERMEDIATE   \n",
      "1  She change picture. Produce owner voice if. Di...                  EXPERT   \n",
      "2  A MERN stack course covers the integration of ...            INTERMEDIATE   \n",
      "3  A React.js course typically covers the fundame...                BEGINNER   \n",
      "4  A machine learning (ML) learning path is a str...                  EXPERT   \n",
      "\n",
      "   course_duration_in_weeks  \n",
      "0                        20  \n",
      "1                         4  \n",
      "2                         6  \n",
      "3                         2  \n",
      "4                         6  \n"
     ]
    }
   ],
   "source": [
    "# COURSE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file\n",
    "courses_data = pd.read_csv('./staging/Course.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_courses_data = courses_data[['course_id', 'course_name', 'description', 'duration', 'difficulty_level']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_courses_data = cleaned_courses_data.drop_duplicates(subset='course_id')\n",
    "\n",
    "# Step 3: Convert duration to weeks\n",
    "def duration_to_weeks(duration):\n",
    "    if 'months' in duration:\n",
    "        return int(duration.split()[0]) * 4  # Assuming 1 month = 4 weeks\n",
    "    elif 'years' in duration:\n",
    "        return int(duration.split()[0]) * 52  # Assuming 1 year = 52 weeks\n",
    "    elif 'weeks' in duration:\n",
    "        return int(duration.split()[0])\n",
    "    else:\n",
    "        return 1  # Handle any unexpected format\n",
    "\n",
    "cleaned_courses_data['duration_in_weeks'] = cleaned_courses_data['duration'].apply(duration_to_weeks)\n",
    "\n",
    "# Step 5: Clean the DataFrame by dropping the original duration column\n",
    "cleaned_courses_data = cleaned_courses_data.drop(columns=['duration'])\n",
    "\n",
    "# step 6: changing column name\n",
    "cleaned_courses_data.rename(columns={'description': 'course_description', 'difficulty_level' : 'course_difficulty_level', 'duration_in_weeks' : 'course_duration_in_weeks'}, inplace=True)\n",
    "\n",
    "# Step 7: Provide information about the cleaned table\n",
    "print(cleaned_courses_data.info())\n",
    "print(cleaned_courses_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_courses_data.to_csv('./prep/cleaned_courses_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   learning_path_id           19 non-null     int64 \n",
      " 1   learning_path_description  19 non-null     object\n",
      " 2   learning_path_name         19 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 588.0+ bytes\n",
      "None\n",
      "   learning_path_id                          learning_path_description  \\\n",
      "0                 1  A machine learning (ML) learning path is a str...   \n",
      "1                 2  An Artificial Intelligence (AI) learning path ...   \n",
      "2                 3  The Full Stack Learning Path equips learners w...   \n",
      "3                 4  The Frontend Learning Path focuses on the desi...   \n",
      "4               100    Master the fundamentals of software development   \n",
      "\n",
      "        learning_path_name  \n",
      "0         Machine Learning  \n",
      "1  Artificial Intelligence  \n",
      "2               Full Stack  \n",
      "3                 Frontend  \n",
      "4     Software Engineering  \n"
     ]
    }
   ],
   "source": [
    "# LEARNING_PATH - TABLE\n",
    "learning_path_data = pd.read_csv('./staging/LearningPath.csv')\n",
    "\n",
    "cleaned_learningPath = learning_path_data[['learning_path_id', 'description', 'path_name']]\n",
    "\n",
    "cleaned_learningPath = cleaned_learningPath.drop_duplicates(subset='learning_path_id')\n",
    "\n",
    "cleaned_learningPath.rename(columns={'path_name' : 'learning_path_name', 'description' : 'learning_path_description'}, inplace=True)\n",
    "\n",
    "print(cleaned_learningPath.info())\n",
    "print(cleaned_learningPath.head())\n",
    "\n",
    "cleaned_learningPath.to_csv('./prep/cleaned_learning_paths.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262 entries, 0 to 261\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   course_id         262 non-null    int64\n",
      " 1   learning_path_id  262 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.2 KB\n",
      "None\n",
      "   course_id  learning_path_id\n",
      "0         12                 1\n",
      "1         12                 2\n",
      "2         10                 3\n",
      "3         10                 4\n",
      "4          9                 3\n"
     ]
    }
   ],
   "source": [
    "# LearningPathMap - TABLE\n",
    "\n",
    "learning_path_map_data = pd.read_csv('./staging/LearningPathMap.csv')\n",
    "\n",
    "cleaned_learningPathMap = learning_path_map_data[['course_id', 'learning_path_id']]\n",
    "\n",
    "cleaned_learningPathMap = cleaned_learningPathMap.drop_duplicates(subset=['course_id', 'learning_path_id'], keep='first')\n",
    "\n",
    "print(cleaned_learningPathMap.info())\n",
    "print(cleaned_learningPathMap.head())\n",
    "\n",
    "cleaned_learningPathMap.to_csv('./prep/cleaned_learning_paths_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333 entries, 0 to 332\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   enroll_id               333 non-null    int64  \n",
      " 1   emp_id                  333 non-null    object \n",
      " 2   course_id               333 non-null    int64  \n",
      " 3   current_page            332 non-null    float64\n",
      " 4   total_pages             332 non-null    float64\n",
      " 5   test_score              332 non-null    float64\n",
      " 6   course_certificate_url  239 non-null    object \n",
      " 7   createdAt               333 non-null    object \n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 20.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333 entries, 0 to 332\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   enroll_id                     333 non-null    int64  \n",
      " 1   emp_id                        333 non-null    object \n",
      " 2   course_id                     333 non-null    int64  \n",
      " 3   createdAt                     333 non-null    object \n",
      " 4   course_certificate_generated  333 non-null    bool   \n",
      " 5   completion_rate               333 non-null    float64\n",
      " 6   test_score_normalized         333 non-null    float64\n",
      "dtypes: bool(1), float64(2), int64(2), object(2)\n",
      "memory usage: 16.1+ KB\n",
      "None\n",
      "   enroll_id  emp_id  course_id                createdAt  \\\n",
      "0          5  JMD003          9  2024-10-11 19:28:53.435   \n",
      "1          3  JMD002          9  2024-10-11 19:28:53.435   \n",
      "2          2  JMD001          9  2024-10-11 19:28:53.435   \n",
      "3          4  JMD001         10  2024-10-11 19:28:53.435   \n",
      "4        100  JMD100        143  2024-03-07 20:45:49.895   \n",
      "\n",
      "   course_certificate_generated  completion_rate  test_score_normalized  \n",
      "0                         False         0.000000                   0.00  \n",
      "1                          True         1.000000                   0.60  \n",
      "2                         False         0.043887                   0.60  \n",
      "3                          True         1.000000                   0.70  \n",
      "4                          True         0.110000                   0.73  \n"
     ]
    }
   ],
   "source": [
    "# CourseEnrollment - TABLE\n",
    "\n",
    "Course_Enrollment_data = pd.read_csv('./staging/CourseEnrollment.csv')\n",
    "\n",
    "cleaned_course_enrollment_data = Course_Enrollment_data[['enroll_id', 'emp_id', 'course_id', 'current_page', 'total_pages', 'test_score', 'course_certificate_url', 'createdAt']]\n",
    "\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop_duplicates(subset=['enroll_id', 'course_id'], keep='first')\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "# Replace missing values without using inplace\n",
    "cleaned_course_enrollment_data['current_page'] = cleaned_course_enrollment_data['current_page'].fillna(0)\n",
    "cleaned_course_enrollment_data['total_pages'] = cleaned_course_enrollment_data['total_pages'].fillna(100)\n",
    "cleaned_course_enrollment_data['test_score'] = cleaned_course_enrollment_data['test_score'].fillna(0)\n",
    "# Create a new boolean column 'course_certificate_generated'\n",
    "cleaned_course_enrollment_data['course_certificate_generated'] = cleaned_course_enrollment_data['course_certificate_url'].apply(lambda x: True if isinstance(x, str) and x.strip() else False)\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop(columns=['course_certificate_url'])\n",
    "\n",
    "# Normalize current_page based on total_pages\n",
    "cleaned_course_enrollment_data['completion_rate'] = cleaned_course_enrollment_data['current_page'] / cleaned_course_enrollment_data['total_pages']\n",
    "# Normalize test_score (assuming the max score is 100)\n",
    "cleaned_course_enrollment_data['test_score_normalized'] = cleaned_course_enrollment_data['test_score'] / 100\n",
    "\n",
    "cleaned_course_enrollment_data.drop(columns=['current_page', 'total_pages', 'test_score'],axis=1, inplace=True)\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "print(cleaned_course_enrollment_data.head())\n",
    "\n",
    "cleaned_course_enrollment_data.to_csv('./prep/cleaned_courseEnrollment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 896 entries, 0 to 895\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   enroll_id          896 non-null    int64 \n",
      " 1   start_time         896 non-null    object\n",
      " 2   time_spent_in_sec  896 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 21.1+ KB\n",
      "None\n",
      "   enroll_id               start_time  time_spent_in_sec\n",
      "0          4  2024-10-05 07:04:42.527                  9\n",
      "1          4  2024-10-05 07:08:06.443                  9\n",
      "2          3  2024-10-05 13:00:54.137                 14\n",
      "3          4  2024-10-05 13:02:45.675                  5\n",
      "4          2  2024-10-05 13:52:37.894                 12\n"
     ]
    }
   ],
   "source": [
    "# CourseEngageLogs - TABLE  \n",
    "\n",
    "CourseEngageLogs = pd.read_csv('./staging/CourseEngageLogs.csv')\n",
    "cleaned_course_engageLogs_data = CourseEngageLogs[['enroll_id', 'start_time', 'time_spent_in_sec']]\n",
    "\n",
    "cleaned_course_engageLogs_data = cleaned_course_engageLogs_data.drop_duplicates(subset=['enroll_id', 'start_time'], keep='first')\n",
    "\n",
    "print(cleaned_course_engageLogs_data.info())\n",
    "print(cleaned_course_engageLogs_data.head())\n",
    "\n",
    "cleaned_course_engageLogs_data.to_csv('./prep/cleaned_courseEngageLogs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1239 entries, 0 to 1238\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   notification_id  1239 non-null   int64 \n",
      " 1   enroll_id        1239 non-null   int64 \n",
      " 2   status           1238 non-null   object\n",
      " 3   user_viewed      1239 non-null   bool  \n",
      " 4   created_date     1239 non-null   object\n",
      "dtypes: bool(1), int64(2), object(2)\n",
      "memory usage: 40.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1239 entries, 0 to 1238\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   notification_id     1239 non-null   int64 \n",
      " 1   enroll_id           1239 non-null   int64 \n",
      " 2   certificate_status  1239 non-null   bool  \n",
      " 3   user_viewed         1239 non-null   bool  \n",
      " 4   created_date        1239 non-null   object\n",
      "dtypes: bool(2), int64(2), object(1)\n",
      "memory usage: 31.6+ KB\n",
      "None\n",
      "   notification_id  enroll_id  certificate_status  user_viewed  \\\n",
      "0                4          3                True         True   \n",
      "1                9          2               False        False   \n",
      "2                5          2               False         True   \n",
      "3                1          4                True         True   \n",
      "4                8          2               False         True   \n",
      "\n",
      "              created_date  \n",
      "0  2024-10-06 00:08:31.000  \n",
      "1  2024-10-06 00:00:26.000  \n",
      "2  2024-10-06 00:50:16.000  \n",
      "3  2024-10-06 00:34:07.000  \n",
      "4  2024-10-06 00:59:13.000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\assad\\AppData\\Local\\Temp\\ipykernel_28648\\3034790909.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "#  Notifications - TABLE\n",
    "\n",
    "notifications_data = pd.read_csv('./staging/Notifications.csv')\n",
    "\n",
    "cleaned_notifications_data = notifications_data[['notification_id', 'enroll_id', 'status', 'user_viewed', 'created_date']]\n",
    "\n",
    "print(cleaned_notifications_data.info())    # status contains null - admin to taken a desition (make it into false)\n",
    "\n",
    "cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n",
    "cleaned_notifications_data.rename(columns={'status': 'certificate_status'}, inplace=True)\n",
    "\n",
    "print(cleaned_notifications_data.info())    \n",
    "print(cleaned_notifications_data.head())\n",
    "\n",
    "cleaned_notifications_data.to_csv('./prep/cleaned_Notifications.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration & Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration and Storage Steps\n",
    "\n",
    "1. **Join Tables**:\n",
    "   - Merge or join different tables to create a unified dataset that includes all necessary features for analysis.\n",
    "   - Ensure that the join keys are appropriate and that the merging process retains the relevant data.\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Create Final Tables:\n",
    "     - Organize the cleaned and transformed data into final tables that are structured for analysis and modeling.\n",
    "     - Save these final tables as CSV files or store them in a database for easy access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "cleaned_employee_data = pd.read_csv('prep/cleaned_employee_data.csv')\n",
    "cleaned_course_enrollment_data = pd.read_csv('prep/cleaned_courseEnrollment.csv')\n",
    "cleaned_courses_data = pd.read_csv('prep/cleaned_courses_data.csv')\n",
    "cleaned_learning_paths_map = pd.read_csv('prep/cleaned_learning_paths_map.csv')\n",
    "cleaned_learning_paths_data = pd.read_csv('prep/cleaned_learning_paths.csv')\n",
    "cleaned_course_engage_logs = pd.read_csv('prep/cleaned_courseEngageLogs.csv')\n",
    "cleaned_notifications = pd.read_csv('prep/cleaned_Notifications.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  total_attempts  accepted_attempts  success_rate\n",
      "0            2               9                  0      0.000000\n",
      "1            3               4                  2      0.500000\n",
      "2            4               5                  2      0.400000\n",
      "3            5               2                  0      0.000000\n",
      "4          100               5                  1      0.200000\n",
      "..         ...             ...                ...           ...\n",
      "328        424               4                  1      0.250000\n",
      "329        425               2                  0      0.000000\n",
      "330        426               3                  1      0.333333\n",
      "331        427               3                  0      0.000000\n",
      "332        428               5                  1      0.200000\n",
      "\n",
      "[333 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "success_rate_df = cleaned_notifications.groupby('enroll_id').agg(\n",
    "    total_attempts=('certificate_status', 'size'),  # Total attempts\n",
    "    accepted_attempts=('certificate_status', lambda x: x.sum()),  # Count of accepted attempts\n",
    ").reset_index()\n",
    "\n",
    "# Calculate success rate\n",
    "success_rate_df['success_rate'] = success_rate_df['accepted_attempts'] / success_rate_df['total_attempts']\n",
    "\n",
    "# Display the results\n",
    "print(success_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  time_spent_in_sec\n",
      "0            2              56151\n",
      "1            3              33402\n",
      "2            4              20491\n",
      "3            5              44259\n",
      "4          100               7877\n",
      "..         ...                ...\n",
      "328        424               8124\n",
      "329        425               1540\n",
      "330        426               7926\n",
      "331        427               9762\n",
      "332        428               7949\n",
      "\n",
      "[333 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by enroll_id and calculate total time spent\n",
    "total_time_spent_df = cleaned_course_engage_logs.groupby('enroll_id')['time_spent_in_sec'].sum().reset_index()\n",
    "\n",
    "# Display the results\n",
    "print(total_time_spent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  emp_id  course_id                createdAt  \\\n",
      "0            5  JMD003          9  2024-10-11 19:28:53.435   \n",
      "1            3  JMD002          9  2024-10-11 19:28:53.435   \n",
      "2            2  JMD001          9  2024-10-11 19:28:53.435   \n",
      "3            4  JMD001         10  2024-10-11 19:28:53.435   \n",
      "4            4  JMD001         10  2024-10-11 19:28:53.435   \n",
      "..         ...     ...        ...                      ...   \n",
      "820        427  JMD198        123  2024-08-18 06:14:29.133   \n",
      "821        427  JMD198        123  2024-08-18 06:14:29.133   \n",
      "822        428  JMD199        155  2023-11-09 21:26:49.803   \n",
      "823        428  JMD199        155  2023-11-09 21:26:49.803   \n",
      "824        428  JMD199        155  2023-11-09 21:26:49.803   \n",
      "\n",
      "     course_certificate_generated  completion_rate  test_score_normalized  \\\n",
      "0                           False         0.000000                   0.00   \n",
      "1                            True         1.000000                   0.60   \n",
      "2                           False         0.043887                   0.60   \n",
      "3                            True         1.000000                   0.70   \n",
      "4                            True         1.000000                   0.70   \n",
      "..                            ...              ...                    ...   \n",
      "820                         False         0.210000                   0.74   \n",
      "821                         False         0.210000                   0.74   \n",
      "822                          True         0.910000                   0.06   \n",
      "823                          True         0.910000                   0.06   \n",
      "824                          True         0.910000                   0.06   \n",
      "\n",
      "                    email             emp_name           designation  ...  \\\n",
      "0     akhil@jmangroup.com                Akhil      SOLUTION_ENABLER  ...   \n",
      "1    pardhu@jmangroup.com               pardhu  SR_SOFTWARE_ENGINEER  ...   \n",
      "2    harsha@jmangroup.com               Harsha     SOFTWARE_ENGINEER  ...   \n",
      "3    harsha@jmangroup.com               Harsha     SOFTWARE_ENGINEER  ...   \n",
      "4    harsha@jmangroup.com               Harsha     SOFTWARE_ENGINEER  ...   \n",
      "..                    ...                  ...                   ...  ...   \n",
      "820  JMD198@jmangroup.com  Johanna Rolfson Jr.     SOFTWARE_ENGINEER  ...   \n",
      "821  JMD198@jmangroup.com  Johanna Rolfson Jr.     SOFTWARE_ENGINEER  ...   \n",
      "822  JMD199@jmangroup.com     Gloria Romaguera  SR_SOFTWARE_ENGINEER  ...   \n",
      "823  JMD199@jmangroup.com     Gloria Romaguera  SR_SOFTWARE_ENGINEER  ...   \n",
      "824  JMD199@jmangroup.com     Gloria Romaguera  SR_SOFTWARE_ENGINEER  ...   \n",
      "\n",
      "                                    course_description  \\\n",
      "0    A MERN stack course covers the integration of ...   \n",
      "1    A MERN stack course covers the integration of ...   \n",
      "2    A MERN stack course covers the integration of ...   \n",
      "3    A React.js course typically covers the fundame...   \n",
      "4    A React.js course typically covers the fundame...   \n",
      "..                                                 ...   \n",
      "820  Television worry information series. Son evide...   \n",
      "821  Television worry information series. Son evide...   \n",
      "822  Stuff water difference seem set. Son pattern M...   \n",
      "823  Stuff water difference seem set. Son pattern M...   \n",
      "824  Stuff water difference seem set. Son pattern M...   \n",
      "\n",
      "    course_difficulty_level course_duration_in_weeks  learning_path_id  \\\n",
      "0              INTERMEDIATE                        6                 3   \n",
      "1              INTERMEDIATE                        6                 3   \n",
      "2              INTERMEDIATE                        6                 3   \n",
      "3                  BEGINNER                        2                 3   \n",
      "4                  BEGINNER                        2                 4   \n",
      "..                      ...                      ...               ...   \n",
      "820            INTERMEDIATE                        4               107   \n",
      "821            INTERMEDIATE                        4               109   \n",
      "822                  EXPERT                       12               100   \n",
      "823                  EXPERT                       12               112   \n",
      "824                  EXPERT                       12               111   \n",
      "\n",
      "                             learning_path_description  \\\n",
      "0    The Full Stack Learning Path equips learners w...   \n",
      "1    The Full Stack Learning Path equips learners w...   \n",
      "2    The Full Stack Learning Path equips learners w...   \n",
      "3    The Full Stack Learning Path equips learners w...   \n",
      "4    The Frontend Learning Path focuses on the desi...   \n",
      "..                                                 ...   \n",
      "820            Develop intelligent systems using AI/ML   \n",
      "821     Connect and control devices with IoT solutions   \n",
      "822    Master the fundamentals of software development   \n",
      "823            Ensure software quality through testing   \n",
      "824                  Master Agile frameworks and tools   \n",
      "\n",
      "           learning_path_name time_spent_in_sec  total_attempts  \\\n",
      "0                  Full Stack             44259               2   \n",
      "1                  Full Stack             33402               4   \n",
      "2                  Full Stack             56151               9   \n",
      "3                  Full Stack             20491               5   \n",
      "4                    Frontend             20491               5   \n",
      "..                        ...               ...             ...   \n",
      "820   AI and Machine Learning              9762               3   \n",
      "821  Internet of Things (IoT)              9762               3   \n",
      "822      Software Engineering              7949               5   \n",
      "823         Quality Assurance              7949               5   \n",
      "824       Agile Methodologies              7949               5   \n",
      "\n",
      "     accepted_attempts  success_rate  \n",
      "0                    0           0.0  \n",
      "1                    2           0.5  \n",
      "2                    0           0.0  \n",
      "3                    2           0.4  \n",
      "4                    2           0.4  \n",
      "..                 ...           ...  \n",
      "820                  0           0.0  \n",
      "821                  0           0.0  \n",
      "822                  1           0.2  \n",
      "823                  1           0.2  \n",
      "824                  1           0.2  \n",
      "\n",
      "[825 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the tables\n",
    "merged_data = (\n",
    "    cleaned_course_enrollment_data\n",
    "    .merge(cleaned_employee_data, on='emp_id', how='left')  # Join Employee Details with Course Enrollment\n",
    "    .merge(cleaned_courses_data, on='course_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(cleaned_learning_paths_map, on='course_id', how='left')  # Join Course Enrollment with Learning Path Mapping\n",
    "    .merge(cleaned_learning_paths_data, on='learning_path_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(total_time_spent_df, on='enroll_id', how='left')  # Join CourseEngageLogs\n",
    "    .merge(success_rate_df, on='enroll_id', how='left')  # Join Notifications\n",
    ")\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_data)\n",
    "\n",
    "merged_data.to_csv('./reporting/merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001             97133                 0.681296            0.666667   \n",
      "1  JMD002             33402                 1.000000            0.600000   \n",
      "2  JMD003             44259                 0.000000            0.000000   \n",
      "3  JMD100             58734                 0.307778            0.666667   \n",
      "4  JMD101             72931                 0.708889            0.072222   \n",
      "\n",
      "   total_certificates  \n",
      "0                   2  \n",
      "1                   1  \n",
      "2                   0  \n",
      "3                   8  \n",
      "4                   9  \n"
     ]
    }
   ],
   "source": [
    "# Group by emp_id to aggregate the features\n",
    "employee_performance = merged_data.groupby('emp_id').agg({\n",
    "    'time_spent_in_sec': 'sum',  # Total Time Spent\n",
    "    'completion_rate': 'mean',  # Average Course Completion\n",
    "    'test_score_normalized': 'mean',  # Average Test Score\n",
    "    'course_certificate_generated': lambda x: (x == True).sum(),  # Count of Generated Certificates\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "employee_performance.rename(columns={\n",
    "    'time_spent_in_sec': 'total_time_spent',\n",
    "    'completion_rate': 'average_completion_rate',\n",
    "    'test_score_normalized': 'average_test_score',\n",
    "    'course_certificate_generated' : 'total_certificates'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the employee performance data\n",
    "print(employee_performance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001             97133                 0.681296            0.666667   \n",
      "1  JMD002             33402                 1.000000            0.600000   \n",
      "2  JMD003             44259                 0.000000            0.000000   \n",
      "3  JMD100             58734                 0.307778            0.666667   \n",
      "4  JMD101             72931                 0.708889            0.072222   \n",
      "\n",
      "   total_certificates  completed_courses_BEGINNER  completed_courses_EXPERT  \\\n",
      "0                   2                           1                         0   \n",
      "1                   1                           0                         0   \n",
      "2                   0                           0                         0   \n",
      "3                   8                           3                         2   \n",
      "4                   9                           1                         1   \n",
      "\n",
      "   completed_courses_INTERMEDIATE  \n",
      "0                               1  \n",
      "1                               1  \n",
      "2                               1  \n",
      "3                               0  \n",
      "4                               1  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on emp_id and course_id to ensure each course is counted once per employee\n",
    "unique_courses = merged_data.drop_duplicates(subset=['emp_id', 'course_id'])\n",
    "# Create a mapping of difficulty levels to counts, grouping by emp_id and course_difficulty_level\n",
    "difficulty_distribution = unique_courses.groupby(['emp_id', 'course_difficulty_level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "difficulty_distribution.columns = [f'completed_courses_{level}' for level in difficulty_distribution.columns]\n",
    "\n",
    "# Merge difficulty distribution with employee performance data\n",
    "employee_performance = employee_performance.merge(difficulty_distribution, on='emp_id', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for completed courses\n",
    "employee_performance.fillna(0, inplace=True)\n",
    "\n",
    "# Display the final employee performance data\n",
    "print(employee_performance.head())\n",
    "employee_performance.to_csv('./reporting/employee_performance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emp_id  learning_path_id        learning_path_name  combined_score\n",
      "0    JMD001                 4                  Frontend        0.665942\n",
      "1    JMD002                 3                Full Stack        0.695184\n",
      "2    JMD003                 3                Full Stack        0.200000\n",
      "3    JMD100               107   AI and Machine Learning        0.523484\n",
      "4    JMD101               102              Data Science        0.321466\n",
      "..      ...               ...                       ...             ...\n",
      "98   JMD195               109  Internet of Things (IoT)        0.651428\n",
      "99   JMD196               107   AI and Machine Learning        0.473529\n",
      "100  JMD197               100      Software Engineering        0.276197\n",
      "101  JMD198               112         Quality Assurance        0.425501\n",
      "102  JMD199               100      Software Engineering        0.272395\n",
      "\n",
      "[103 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the learning path predictions\n",
    "learning_path_performance = (\n",
    "    merged_data.groupby(['emp_id', 'learning_path_id'])\n",
    "    .agg({\n",
    "        'completion_rate': 'mean',  # Average completion percentage\n",
    "        'success_rate': 'mean',\n",
    "        'time_spent_in_sec': 'mean',\n",
    "        'test_score_normalized': 'mean'        \n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Min-Max normalization\n",
    "min_time_spent = learning_path_performance['time_spent_in_sec'].min()\n",
    "max_time_spent = learning_path_performance['time_spent_in_sec'].max()\n",
    "\n",
    "\n",
    "# Calculate the ratio or combined score\n",
    "learning_path_performance['combined_score'] = (\n",
    "    (learning_path_performance['completion_rate'] * 0.2) + \n",
    "    ((\n",
    "        (learning_path_performance['time_spent_in_sec'] - min_time_spent) /\n",
    "        (max_time_spent - min_time_spent)\n",
    "    )* 0.2) + \n",
    "    (learning_path_performance['success_rate'] * 0.15) + \n",
    "    (learning_path_performance['test_score_normalized'] * 0.45)\n",
    ")\n",
    "\n",
    "# Get the learning path with the highest combined score for each employee\n",
    "best_learning_paths = learning_path_performance.loc[learning_path_performance.groupby('emp_id')['combined_score'].idxmax()]\n",
    "\n",
    "# Merge with learning path details to get descriptions\n",
    "best_learning_paths = best_learning_paths.merge(cleaned_learning_paths_data[['learning_path_id', 'learning_path_name']], on='learning_path_id', how='left')\n",
    "\n",
    "# Display the final recommendations\n",
    "print(best_learning_paths[['emp_id', 'learning_path_id', 'learning_path_name', 'combined_score']])\n",
    "best_learning_paths.to_csv('./reporting/best_learning_paths.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
